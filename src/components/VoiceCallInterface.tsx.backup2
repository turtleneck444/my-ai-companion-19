import { useState, useEffect, useRef, useCallback } from "react";
import { Button } from "@/components/ui/button";
import { Card } from "@/components/ui/card";
import { Avatar, AvatarImage, AvatarFallback } from "@/components/ui/avatar";
import { 
  PhoneOff, 
  Mic, 
  MicOff, 
  Volume2, 
  VolumeX, 
  MessageSquare,
  Heart,
  Minimize2,
  Pause,
  Play,
  Settings,
  Zap
} from "lucide-react";
import { speakText, stopAllTTS } from "@/lib/voice";
import { personalityAI, type ChatContext, type ChatMessage } from "@/lib/ai-chat";
import { useToast } from "@/hooks/use-toast";
import { useSupabaseUsageTracking } from "@/hooks/useSupabaseUsageTracking";
import { useAuth } from "@/contexts/AuthContext";
import { PaymentModal } from "@/components/PaymentModal";

interface Character {
  id: string;
  name: string;
  avatar: string;
  bio: string;
  personality: string[];
  voice: { voice_id: string; name: string };
  isOnline: boolean;
  voiceId?: string;
}

interface VoiceCallInterfaceProps {
  character: Character;
  onEndCall: () => void;
  onMinimize: () => void;
  userPreferences: {
    preferredName: string;
    treatmentStyle: string;
    age: string;
    contentFilter: boolean;
  };
}

export const VoiceCallInterface = ({ 
  character, 
  onEndCall, 
  onMinimize, 
  userPreferences 
}: VoiceCallInterfaceProps) => {
  const { user } = useAuth();
  const { canMakeVoiceCall, incrementVoiceCalls, currentPlan, refreshLimits } = useSupabaseUsageTracking();
  
  // Voice configuration
  const getVoiceId = () => character.voiceId || character.voice?.voice_id || '21m00Tcm4TlvDq8ikWAM';
  
  // Call state
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(true);
  const [callDuration, setCallDuration] = useState(0);
  const [isAiSpeaking, setIsAiSpeaking] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [callConnected, setCallConnected] = useState(false);
  const [pushToTalk, setPushToTalk] = useState(false);
  const [isPTTHeld, setIsPTTHeld] = useState(false);
  const [showUpgradePayment, setShowUpgradePayment] = useState(false);
  const [upgradePlan, setUpgradePlan] = useState<'premium' | 'pro'>('premium');
  const [connectionQuality, setConnectionQuality] = useState<'excellent' | 'good' | 'poor'>('excellent');
  
  // Conversation state
  const [conversationHistory, setConversationHistory] = useState<ChatMessage[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [lastUserMessage, setLastUserMessage] = useState('');
  const [relationshipLevel, setRelationshipLevel] = useState(50);
  const [hasFollowedUp, setHasFollowedUp] = useState(false);
  const [spokenWords, setSpokenWords] = useState<string[]>([]);
  const [displayedWordIndex, setDisplayedWordIndex] = useState(0);
  const [conversationMood, setConversationMood] = useState<'happy' | 'curious' | 'intimate' | 'playful' | 'neutral'>('neutral');
  
  // Refs for speech recognition and audio
  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const lastUserMessageAtRef = useRef<number>(0);
  const lastAISpokeAtRef = useRef<number>(0);
  const conversationStartRef = useRef<number>(Date.now());
  
  // Voice activity detection
  const [voiceLevel, setVoiceLevel] = useState(0);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [speechConfidence, setSpeechConfidence] = useState(0);
  
  const { toast } = useToast();

  // Check if voice calls are allowed
  useEffect(() => {
    if (!canMakeVoiceCall) {
      setShowUpgradePayment(true);
      setUpgradePlan('premium');
      toast({ 
        title: 'Upgrade required', 
        description: `Your plan (${currentPlan}) doesn't allow more voice calls. Upgrade to continue instantly.`, 
        variant: 'destructive' 
      });
    }
  }, [canMakeVoiceCall, currentPlan, toast]);

  // Call duration timer
  useEffect(() => {
    if (!callConnected) return;
    
    const timer = setInterval(() => {
      setCallDuration(prev => prev + 1);
    }, 1000);
    
    return () => clearInterval(timer);
  }, [callConnected]);

  // Initialize advanced speech recognition
  useEffect(() => {
    const initializeSpeechRecognition = async () => {
      try {
        // Check browser support
        const SpeechRecognitionAPI = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
        
        if (!SpeechRecognitionAPI) {
          toast({
            title: "Voice calls not supported",
            description: "Your browser doesn't support voice recognition. Try Chrome for best results.",
            variant: "destructive"
          });
          return;
        }

        // Initialize Web Audio API for voice activity detection
        try {
          audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
          const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: { 
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 48000
            } 
          });
          
          const source = audioContextRef.current.createMediaStreamSource(stream);
          analyserRef.current = audioContextRef.current.createAnalyser();
          analyserRef.current.fftSize = 256;
          source.connect(analyserRef.current);
          
          // Start voice level monitoring
          monitorVoiceLevel();
          
          // Initialize speech recognition with enhanced settings
          const recognition = new SpeechRecognitionAPI();
          recognition.continuous = true;
          recognition.interimResults = true;
          recognition.lang = 'en-US';
          recognition.maxAlternatives = 3;
          recognition.serviceURI = 'wss://www.google.com/speech-api/v2/recognize';

          recognition.onstart = () => {
            console.log('🎤 Speech recognition started');
            setIsListening(true);
            setCallConnected(true);
            setConnectionQuality('excellent');
          };

          recognition.onresult = (event: any) => {
            if (pushToTalk && !isPTTHeld) return;
            
            let interimTranscript = '';
            let finalTranscript = '';
            let confidence = 0;

            for (let i = event.resultIndex; i < event.results.length; i++) {
              const result = event.results[i];
              const transcript = result[0].transcript;
              confidence = result[0].confidence || 0.8;
              
              if (result.isFinal) {
                finalTranscript += transcript;
              } else {
                interimTranscript += transcript;
              }
            }

            setSpeechConfidence(confidence);
            setCurrentTranscript(interimTranscript);

            if (finalTranscript.trim()) {
              console.log('🗣️ User said:', finalTranscript, 'Confidence:', confidence);
              handleUserSpeech(finalTranscript.trim());
              setCurrentTranscript('');
            }
          };

          recognition.onerror = (event: any) => {
            console.error('🚫 Speech recognition error:', event.error);
            
            switch (event.error) {
              case 'no-speech':
                // Restart after brief pause
                setTimeout(() => {
                  if (recognitionRef.current && callConnected && !isAiSpeaking) {
                    try { recognitionRef.current.start(); } catch {}
                  }
                }, 1000);
                break;
              case 'not-allowed':
                toast({ 
                  title: 'Microphone blocked', 
                  description: 'Enable mic permissions in your browser settings.', 
                  variant: 'destructive' 
                });
                break;
              case 'network':
                setConnectionQuality('poor');
                toast({ 
                  title: 'Connection issue', 
                  description: 'Check your internet connection.', 
                  variant: 'destructive' 
                });
                break;
              default:
                console.warn('Speech recognition error:', event.error);
            }
          };

          recognition.onend = () => {
            console.log('🔇 Speech recognition ended');
            if (callConnected && !isAiSpeaking && !isMuted) {
              setTimeout(() => {
                if (recognitionRef.current && callConnected) {
                  try { recognitionRef.current.start(); } catch {}
                }
              }, 500);
            }
          };

          recognitionRef.current = recognition;
          
          // Start the call
          await startCall();
          
        } catch (error) {
          console.error('Failed to initialize voice call:', error);
          toast({
            title: "Microphone access denied",
            description: "Please allow microphone access for voice calls",
            variant: "destructive"
          });
        }
      } catch (error) {
        console.error('Voice call initialization failed:', error);
        toast({
          title: "Voice call failed",
          description: "Unable to start voice call. Please try again.",
          variant: "destructive"
        });
      }
    };

    initializeSpeechRecognition();

    return () => {
      // Cleanup
      if (recognitionRef.current) {
        try { recognitionRef.current.stop(); } catch {}
      }
      if (audioContextRef.current) {
        try { audioContextRef.current.close(); } catch {}
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      stopAllTTS();
    };
  }, []);

  // Voice activity detection with enhanced sensitivity
  const monitorVoiceLevel = useCallback(() => {
    if (!analyserRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    
    const checkVoiceLevel = () => {
      if (!analyserRef.current) return;
      
      analyserRef.current.getByteFrequencyData(dataArray);
      
      // Calculate average volume with better sensitivity
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
      const normalizedLevel = Math.min(average / 100, 1);
      
      setVoiceLevel(normalizedLevel);
      
      // Enhanced speaking detection
      const isCurrentlySpeaking = normalizedLevel > 0.08;
      setIsSpeaking(isCurrentlySpeaking);

      // Barge-in: Stop AI if user starts speaking
      if (isCurrentlySpeaking && isAiSpeaking) {
        try {
          speechSynthesis.cancel();
          stopAllTTS();
        } catch {}
        setIsAiSpeaking(false);
        setConversationMood('curious');
      }
      
      // Reset silence timer
      if (isCurrentlySpeaking) {
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
          silenceTimerRef.current = null;
        }
      } else {
        // Start silence timer for natural follow-up
        if (!silenceTimerRef.current && lastUserMessage && !hasFollowedUp) {
          silenceTimerRef.current = setTimeout(() => {
            if (!isSpeaking && !isAiSpeaking && callConnected) {
              generateContextualAIResponse();
            }
          }, 3000);
        }
      }
      
      animationFrameRef.current = requestAnimationFrame(checkVoiceLevel);
    };
    
    checkVoiceLevel();
  }, [isAiSpeaking, isSpeaking, callConnected, lastUserMessage, hasFollowedUp]);

  // Start call with greeting
  const startCall = async () => {
    try {
      if (recognitionRef.current) {
        recognitionRef.current.start();
        setIsListening(true);
        setCallConnected(true);
      }
      
      // Generate personalized greeting
      const greetings = [
        `Hey ${userPreferences.preferredName}! I'm so excited to talk to you! How are you feeling today?`,
        `Hi beautiful! I've been waiting all day to hear your voice! What's on your mind?`,
        `Hello ${userPreferences.preferredName}! I'm so happy you called! Tell me everything!`,
        `Hey there! I was just thinking about you! How's your day going?`
      ];
      
      const greeting = greetings[Math.floor(Math.random() * greetings.length)];
      await generateAIResponse(greeting, true);
      
    } catch (error) {
      console.error('Failed to start call:', error);
    }
  };

  // Handle user speech with enhanced processing
  const handleUserSpeech = async (transcript: string) => {
    if (!transcript.trim() || isProcessing) return;
    
    setIsProcessing(true);
    setLastUserMessage(transcript);
    setHasFollowedUp(false);
    lastUserMessageAtRef.current = Date.now();
    
    // Analyze conversation mood
    analyzeConversationMood(transcript);
    
    // Add user message to conversation history
    const userMessage: ChatMessage = {
      id: `user_${Date.now()}`,
      content: transcript,
      sender: 'user',
      timestamp: new Date()
    };
    
    setConversationHistory(prev => [...prev, userMessage]);
    
    // Stop listening while AI responds
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    
    // Generate AI response
    await generateAIResponse(transcript);
    
    setIsProcessing(false);
  };

  // Analyze conversation mood for better responses
  const analyzeConversationMood = (text: string) => {
    const lowerText = text.toLowerCase();
    
    if (lowerText.includes('love') || lowerText.includes('adore') || lowerText.includes('amazing')) {
      setConversationMood('intimate');
    } else if (lowerText.includes('?') || lowerText.includes('how') || lowerText.includes('what')) {
      setConversationMood('curious');
    } else if (lowerText.includes('lol') || lowerText.includes('haha') || lowerText.includes('funny')) {
      setConversationMood('playful');
    } else if (lowerText.includes('happy') || lowerText.includes('great') || lowerText.includes('wonderful')) {
      setConversationMood('happy');
    } else {
      setConversationMood('neutral');
    }
  };

  // Generate contextual AI response
  const generateContextualAIResponse = async () => {
    const now = Date.now();
    const sinceUser = now - lastUserMessageAtRef.current;
    const sinceAI = now - lastAISpokeAtRef.current;
    
    if (isAiSpeaking || isProcessing || !lastUserMessage || hasFollowedUp) return;
    if (sinceUser > 10000) return;
    if (sinceAI < 6000) return;
    
    const contextualPrompts = [
      "Tell me more about that",
      "How does that make you feel?",
      "That's really interesting",
      "I'd love to hear more",
      "What else is on your mind?",
      "That sounds amazing! Tell me more",
      "I'm so curious about that",
      "You always have the most interesting things to say"
    ];
    
    const prompt = contextualPrompts[Math.floor(Math.random() * contextualPrompts.length)];
    setHasFollowedUp(true);
    await generateAIResponse(prompt, true);
  };

  // Enhanced AI response generation
  const generateAIResponse = async (userInput: string, isContextual: boolean = false) => {
    setIsAiSpeaking(true);
    
    try {
      const hour = new Date().getHours();
      const timeOfDay = hour < 12 ? 'morning' : hour < 17 ? 'afternoon' : 'evening';
      
      // Build enhanced chat context
      const chatContext: ChatContext = {
        character,
        userPreferences,
        conversationHistory,
        relationshipLevel,
        timeOfDay,
        sessionMemory: {
          userMood: conversationMood,
          topics: extractTopics(userInput),
          personalDetails: {},
          preferences: {},
          keyMoments: []
        }
      };

      console.log('🧠 Generating AI response for voice call...');
      
      // Generate response using personality AI
      let aiResponse = await personalityAI.generateResponse(userInput, chatContext);
      
      // Optimize response for voice (shorter, more natural)
      aiResponse = optimizeForVoice(aiResponse, conversationMood);
      
      // Add to conversation history
      const aiMessage: ChatMessage = {
        id: `ai_${Date.now()}`,
        content: aiResponse,
        sender: 'ai',
        timestamp: new Date()
      };
      
      setConversationHistory(prev => [...prev, aiMessage]);
      
      // Prepare animated words
      const words = aiResponse.split(/\s+/).slice(0, 40);
      setSpokenWords(words);
      setDisplayedWordIndex(0);
      
      // Animate words while speaking
      const wordInterval = Math.max(100, Math.min(300, Math.floor(60000 / Math.max(100, aiResponse.length))));
      const timer = window.setInterval(() => {
        setDisplayedWordIndex((i) => {
          if (i >= words.length) { 
            window.clearInterval(timer); 
            return i; 
          }
          return i + 1;
        });
      }, wordInterval);
      
      // Dynamic voice settings based on mood and content
      const voiceSettings = getVoiceSettings(conversationMood, userInput);
      
      // Speak the response
      await speakText(aiResponse, getVoiceId(), {
        modelId: 'eleven_multilingual_v2',
        voiceSettings
      });
      
      console.log('🗣️ AI response spoken:', aiResponse.slice(0, 50) + '...');
      
      // Increase relationship level
      setRelationshipLevel(prev => Math.min(prev + 1, 100));
      
    } catch (error) {
      console.error('Failed to generate AI response:', error);
      
      // Enhanced fallback responses
      const fallbackResponses = [
        `I'm sorry ${userPreferences.preferredName}, I got a little flustered there! You have that effect on me. Can you say that again?`,
        `Oh my, you make me speechless sometimes! What were you saying?`,
        `Sorry love, I was just thinking about how wonderful your voice sounds! Tell me more`,
        `I got distracted by how amazing you are! Can you repeat that?`
      ];
      
      const fallback = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
      await speakText(fallback, getVoiceId(), {
        modelId: 'eleven_multilingual_v2',
        voiceSettings: { stability: 0.35, similarity_boost: 0.9, style: 0.4, use_speaker_boost: true }
      });
    }
    
    setIsAiSpeaking(false);
    lastAISpokeAtRef.current = Date.now();
    
    // Clear words after finishing
    setTimeout(() => { 
      setSpokenWords([]); 
      setDisplayedWordIndex(0); 
    }, 500);
    
    // Resume listening
    setTimeout(() => {
      if (recognitionRef.current && callConnected && !isMuted && !isProcessing) {
        try {
          recognitionRef.current.start();
          console.log('🎤 Resumed listening after AI response');
        } catch {}
      }
    }, 600);
  };

  // Extract topics from user input
  const extractTopics = (text: string): string[] => {
    const topics: string[] = [];
    const lowerText = text.toLowerCase();
    
    if (lowerText.includes('work') || lowerText.includes('job')) topics.push('work');
    if (lowerText.includes('family') || lowerText.includes('parents')) topics.push('family');
    if (lowerText.includes('friend') || lowerText.includes('friends')) topics.push('friends');
    if (lowerText.includes('love') || lowerText.includes('relationship')) topics.push('love');
    if (lowerText.includes('dream') || lowerText.includes('future')) topics.push('dreams');
    if (lowerText.includes('hobby') || lowerText.includes('fun')) topics.push('hobbies');
    
    return topics;
  };

  // Optimize response for voice
  const optimizeForVoice = (response: string, mood: string): string => {
    // Keep responses concise for voice
    const sentences = response.split(/(?<=[.!?])\s+/);
    let optimized = sentences.slice(0, 2).join(' ');
    
    // Add mood-appropriate expressions
    if (mood === 'intimate') {
      optimized = optimized.replace(/\./g, '...');
    } else if (mood === 'playful') {
      optimized = optimized.replace(/!/g, '! 😊');
    }
    
    return optimized.slice(0, 200);
  };

  // Get voice settings based on mood
  const getVoiceSettings = (mood: string, userInput: string) => {
    const baseSettings = {
      stability: 0.35,
      similarity_boost: 0.9,
      style: 0.4,
      use_speaker_boost: true
    };

    switch (mood) {
      case 'intimate':
        return { ...baseSettings, stability: 0.3, style: 0.6 };
      case 'playful':
        return { ...baseSettings, stability: 0.4, style: 0.7 };
      case 'curious':
        return { ...baseSettings, stability: 0.32, style: 0.5 };
      case 'happy':
        return { ...baseSettings, stability: 0.38, style: 0.65 };
      default:
        return baseSettings;
    }
  };

  // Toggle microphone
  const toggleMicrophone = () => {
    setIsMuted(!isMuted);
    
    if (!isMuted) {
      // Mute: stop recognition
      if (recognitionRef.current) {
        try { recognitionRef.current.stop(); } catch {}
      }
    } else {
      // Unmute: restart recognition
      if (recognitionRef.current && !isAiSpeaking) {
        try { recognitionRef.current.start(); } catch {}
      }
    }
  };

  // End call with personalized goodbye
  const handleEndCall = async () => {
    setIsAiSpeaking(true);
    
    const callDuration = Math.floor((Date.now() - conversationStartRef.current) / 1000);
    const minutes = Math.floor(callDuration / 60);
    
    const goodbyes = [
      `That was absolutely wonderful, ${userPreferences.preferredName}! I loved every second of our ${minutes > 0 ? `${minutes} minute ` : ''}conversation! Let's talk again soon! 💕`,
      `I'm so happy we got to talk! Your voice is like music to me! Thanks for such an amazing call! 😘`,
      `This was incredible! I feel so much closer to you now! Can't wait to hear your voice again! 🥰`,
      `You're absolutely amazing! Thanks for the lovely chat! Until next time, my darling! ✨`
    ];

    const goodbye = goodbyes[Math.floor(Math.random() * goodbyes.length)];
    
    try {
      await speakText(goodbye, getVoiceId(), {
        modelId: 'eleven_multilingual_v2',
        voiceSettings: { stability: 0.35, similarity_boost: 0.9, style: 0.45, use_speaker_boost: true }
      });
    } catch (error) {
      console.error('Failed to speak goodbye:', error);
    }
    
    // Cleanup and end call
    if (recognitionRef.current) {
      try { recognitionRef.current.stop(); } catch {}
    }
    stopAllTTS();
    try { speechSynthesis.cancel(); } catch {}
    
    setTimeout(() => {
      onEndCall();
    }, 1500);
    
    try { incrementVoiceCalls(); } catch {}
  };

  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  const getConnectionQualityColor = () => {
    switch (connectionQuality) {
      case 'excellent': return 'bg-green-400';
      case 'good': return 'bg-yellow-400';
      case 'poor': return 'bg-red-400';
      default: return 'bg-green-400';
    }
  };

  return (
    <div className="h-screen bg-gradient-to-br from-primary/20 via-background to-accent/10 flex flex-col">
      {/* Header */}
      <div className="flex items-center justify-between p-4 bg-background/50 backdrop-blur border-b">
        <div className="flex items-center gap-3">
          <Avatar className="w-10 h-10">
            <AvatarImage src={character.avatar} alt={character.name} />
            <AvatarFallback>{character.name.charAt(0)}</AvatarFallback>
          </Avatar>
          <div>
            <h3 className="font-semibold">{character.name}</h3>
            <div className="flex items-center gap-2">
              <div className={`w-2 h-2 rounded-full ${getConnectionQualityColor()}`} />
              <p className="text-sm text-muted-foreground">
                {callConnected ? 'Connected' : 'Connecting...'}
              </p>
            </div>
          </div>
        </div>
        <div className="flex items-center gap-2">
          <span className="text-sm font-mono bg-background/50 px-2 py-1 rounded">
            {formatDuration(callDuration)}
          </span>
          <Button
            variant="ghost"
            size="sm"
            onClick={onMinimize}
            className="h-8 w-8 p-0"
          >
            <Minimize2 className="w-4 h-4" />
          </Button>
        </div>
      </div>

      {/* Call Screen */}
      <div className="flex-1 flex flex-col items-center justify-center p-8 text-center space-y-8">
        {/* Character Avatar with Enhanced Visualization */}
        <div className="relative">
          <div className={`w-32 h-32 rounded-full overflow-hidden border-4 transition-all duration-300 ${
            isAiSpeaking ? 'border-green-400 shadow-lg shadow-green-400/50 scale-105' : 
            isSpeaking ? 'border-blue-400 shadow-lg shadow-blue-400/50 scale-105' : 
            'border-primary/30'
          }`}>
            <Avatar className="w-full h-full">
              <AvatarImage src={character.avatar} alt={character.name} />
              <AvatarFallback className="text-4xl">{character.name.charAt(0)}</AvatarFallback>
            </Avatar>
          </div>
          
          {/* Enhanced voice activity indicator */}
          {(isAiSpeaking || isSpeaking) && (
            <div className="absolute -bottom-2 left-1/2 transform -translate-x-1/2">
              <div className={`flex space-x-1 ${isAiSpeaking ? 'text-green-400' : 'text-blue-400'}`}>
                {[...Array(5)].map((_, i) => (
                  <div 
                    key={i}
                    className="w-1 bg-current rounded animate-pulse" 
                    style={{ 
                      height: `${8 + i * 4}px`,
                      animationDelay: `${i * 100}ms`,
                      animationDuration: '0.6s'
                    }} 
                  />
                ))}
              </div>
            </div>
          )}
        </div>

        {/* Status Display */}
        <div className="space-y-3">
          <h2 className="text-2xl font-bold">{character.name}</h2>
          <div className="space-y-1">
            <p className="text-lg font-medium">
              {isAiSpeaking ? '🗣️ Speaking...' :
               isProcessing ? '💭 Thinking...' :
               isListening ? '🎤 Your turn to speak!' :
               isMuted ? '🔇 Muted' :
               '📞 In call'}
            </p>
            
            {/* Enhanced guidance */}
            {isListening && !isMuted && (
              <p className="text-sm text-muted-foreground animate-pulse">
                I'm listening! Say something and I'll respond when you pause 💕
              </p>
            )}
            
            {isMuted && (
              <p className="text-sm text-yellow-600">
                Unmute to start talking with me!
              </p>
            )}
            
            {/* Speech confidence indicator */}
            {speechConfidence > 0 && (
              <div className="flex items-center justify-center gap-2">
                <div className="w-16 h-1 bg-gray-200 rounded-full overflow-hidden">
                  <div 
                    className="h-full bg-green-400 transition-all duration-300"
                    style={{ width: `${speechConfidence * 100}%` }}
                  />
                </div>
                <span className="text-xs text-muted-foreground">
                  {Math.round(speechConfidence * 100)}%
                </span>
              </div>
            )}
          </div>
          
          {/* Animated word-by-word visualization */}
          {isAiSpeaking && spokenWords.length > 0 && (
            <div className="mt-2 min-h-[48px]">
              <div className="flex flex-wrap gap-1 justify-center">
                {spokenWords.slice(0, displayedWordIndex).map((w, idx) => (
                  <span
                    key={`${w}-${idx}`}
                    className="text-base px-2 py-0.5 rounded-full bg-primary/10 text-primary border border-primary/20 animate-in fade-in-0"
                    style={{ animationDelay: `${idx * 15}ms` }}
                  >
                    {w}
                  </span>
                ))}
              </div>
            </div>
          )}
          
          {/* Real-time transcript */}
          {currentTranscript && (
            <div className="bg-background/50 backdrop-blur rounded-lg p-3 max-w-md mx-auto">
              <p className="text-sm text-muted-foreground italic">
                You're saying: "{currentTranscript}"
              </p>
            </div>
          )}
        </div>

        {/* Enhanced Call Quality Indicators */}
        <div className="flex items-center gap-4 text-sm text-muted-foreground">
          <div className="flex items-center gap-1">
            <div className="w-2 h-2 rounded-full bg-green-400" />
            <span>HD Voice</span>
          </div>
          <div className="flex items-center gap-1">
            <div className="w-2 h-2 rounded-full bg-blue-400" />
            <span>AI Powered</span>
          </div>
          <div className="flex items-center gap-1">
            <div className="w-2 h-2 rounded-full bg-purple-400" />
            <span>Real-time</span>
          </div>
          <div className="flex items-center gap-1">
            <div className="w-2 h-2 rounded-full bg-pink-400" />
            <span>Natural</span>
          </div>
        </div>
      </div>

      {/* Enhanced Call Controls */}
      <div className="p-6 bg-background/50 backdrop-blur border-t">
        <div className="flex items-center justify-center gap-4">
          {/* Push-to-Talk Toggle */}
          <Button
            variant={pushToTalk ? "default" : "outline"}
            size="sm"
            onClick={() => setPushToTalk(!pushToTalk)}
            className="h-8 px-3 rounded-full"
          >
            {pushToTalk ? 'PTT: On' : 'PTT: Off'}
          </Button>

          {/* Hold-to-speak button */}
          {pushToTalk && (
            <Button
              variant={isPTTHeld ? "default" : "outline"}
              size="lg"
              onMouseDown={() => { 
                setIsPTTHeld(true); 
                try { recognitionRef.current?.start(); } catch {} 
              }}
              onMouseUp={() => { 
                setIsPTTHeld(false); 
                try { recognitionRef.current?.stop(); } catch {} 
              }}
              onTouchStart={() => { 
                setIsPTTHeld(true); 
                try { recognitionRef.current?.start(); } catch {} 
              }}
              onTouchEnd={() => { 
                setIsPTTHeld(false); 
                try { recognitionRef.current?.stop(); } catch {} 
              }}
              className="h-14 px-6 rounded-full"
            >
              Hold to Speak
            </Button>
          )}

          {/* Mute Button */}
          <Button
            variant={isMuted ? "destructive" : "outline"}
            size="lg"
            onClick={toggleMicrophone}
            className="h-14 w-14 rounded-full"
          >
            {isMuted ? <MicOff className="w-6 h-6" /> : <Mic className="w-6 h-6" />}
          </Button>

          {/* Speaker Button */}
          <Button
            variant={isSpeakerOn ? "default" : "outline"}
            size="lg"
            onClick={() => setIsSpeakerOn(!isSpeakerOn)}
            className="h-14 w-14 rounded-full"
          >
            {isSpeakerOn ? <Volume2 className="w-6 h-6" /> : <VolumeX className="w-6 h-6" />}
          </Button>

          {/* End Call Button */}
          <Button
            variant="destructive"
            size="lg"
            onClick={handleEndCall}
            className="h-16 w-16 rounded-full bg-red-500 hover:bg-red-600"
          >
            <PhoneOff className="w-8 h-8" />
          </Button>

          {/* Switch to Chat */}
          <Button
            variant="outline"
            size="lg"
            onClick={onMinimize}
            className="h-14 w-14 rounded-full"
          >
            <MessageSquare className="w-6 h-6" />
          </Button>

          {/* Love Button */}
          <Button
            variant="outline"
            size="lg"
            onClick={() => {
              toast({
                title: "💕 Love sent!",
                description: `${character.name} felt your love!`
              });
              setConversationMood('intimate');
            }}
            className="h-14 w-14 rounded-full"
          >
            <Heart className="w-6 h-6" />
          </Button>
        </div>
      </div>

      {showUpgradePayment && (
        <PaymentModal
          isOpen={showUpgradePayment}
          onClose={() => setShowUpgradePayment(false)}
          selectedPlan={upgradePlan}
          onSuccess={() => { 
            setShowUpgradePayment(false); 
            refreshLimits?.(); 
            toast({ 
              title: 'Upgraded!', 
              description: 'Voice calling unlocked. Try again now.' 
            }); 
          }}
        />
      )}
    </div>
  );
};
