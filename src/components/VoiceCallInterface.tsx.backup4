import React, { useState, useEffect, useRef, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Card } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import { Separator } from '@/components/ui/separator';
import { useToast } from '@/hooks/use-toast';
import { speakText, stopAllTTS, isTTSPlaying, getNaturalVoiceSettings } from '@/lib/voice';
import { personalityAI, ChatMessage, Character, UserPreferences, ChatContext } from '@/lib/ai-chat';
import { 
  Mic, 
  MicOff, 
  Volume2, 
  VolumeX, 
  Phone, 
  PhoneOff, 
  Loader2,
  Heart,
  MessageCircle,
  Zap,
  Brain,
  Eye,
  Ear,
  Activity
} from 'lucide-react';

interface VoiceCallInterfaceProps {
  character: Character;
  userPreferences: UserPreferences;
  onEndCall: () => void;
  className?: string;
}

interface ConversationState {
  isListening: boolean;
  isProcessing: boolean;
  isSpeaking: boolean;
  currentMessage: string;
  conversationHistory: ChatMessage[];
  relationshipLevel: number;
  sessionMemory: any;
  mood: string;
  energy: number;
  connectionQuality: number;
}

export const VoiceCallInterface: React.FC<VoiceCallInterfaceProps> = ({
  character,
  userPreferences,
  onEndCall,
  className = ''
}) => {
  const { toast } = useToast();
  
  // Enhanced state management
  const [state, setState] = useState<ConversationState>({
    isListening: false,
    isProcessing: false,
    isSpeaking: false,
    currentMessage: '',
    conversationHistory: [],
    relationshipLevel: 50,
    sessionMemory: {},
    mood: 'neutral',
    energy: 70,
    connectionQuality: 95
  });

  // Refs for audio processing
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationRef = useRef<number | null>(null);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const conversationTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const isCallActiveRef = useRef<boolean>(true); // Track if call is still active

  // Enhanced speech recognition setup
  const setupSpeechRecognition = useCallback(() => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      toast({
        title: "Speech Recognition Not Supported",
        description: "Your browser doesn't support speech recognition. Please use Chrome or Edge.",
        variant: "destructive"
      });
      return false;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    const recognition = new SpeechRecognition();
    
    // Enhanced recognition settings for better accuracy
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'en-US';
    recognition.maxAlternatives = 3;
    recognition.serviceURI = 'wss://www.google.com/speech-api/v2/recognize';
    
    recognition.onstart = () => {
      console.log('🎤 Speech recognition started');
      if (isCallActiveRef.current) {
        setState(prev => ({ ...prev, isListening: true }));
      }
    };

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      if (!isCallActiveRef.current) return; // Don't process if call ended
      
      let finalTranscript = '';
      let interimTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (finalTranscript && isCallActiveRef.current) {
        console.log('🎯 Final transcript:', finalTranscript);
        setState(prev => ({ ...prev, currentMessage: finalTranscript }));
        handleUserMessage(finalTranscript);
      } else if (interimTranscript && isCallActiveRef.current) {
        console.log('🔄 Interim transcript:', interimTranscript);
        setState(prev => ({ ...prev, currentMessage: interimTranscript }));
      }
    };

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error('❌ Speech recognition error:', event.error);
      if (isCallActiveRef.current) {
        setState(prev => ({ ...prev, isListening: false, isProcessing: false }));
        
        if (event.error === 'no-speech' && isCallActiveRef.current) {
          // Restart recognition after brief pause
          setTimeout(() => {
            if (!state.isSpeaking && isCallActiveRef.current) {
              startListening();
            }
          }, 1000);
        }
      }
    };

    recognition.onend = () => {
      console.log('🛑 Speech recognition ended');
      if (isCallActiveRef.current) {
        setState(prev => ({ ...prev, isListening: false }));
        
        // Restart if not speaking and not manually stopped
        if (!state.isSpeaking && !state.isProcessing && isCallActiveRef.current) {
          setTimeout(() => {
            if (isCallActiveRef.current) {
              startListening();
            }
          }, 500);
        }
      }
    };

    recognitionRef.current = recognition;
    return true;
  }, [state.isSpeaking, state.isProcessing]);

  // Enhanced audio context setup for voice activity detection
  const setupAudioContext = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: { 
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 44100
        } 
      });
      
      audioContextRef.current = new AudioContext();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      
      analyserRef.current.fftSize = 256;
      analyserRef.current.smoothingTimeConstant = 0.8;
      
      source.connect(analyserRef.current);
      
      console.log('🎵 Audio context setup complete');
      return true;
    } catch (error) {
      console.error('❌ Audio context setup failed:', error);
      return false;
    }
  }, []);

  // Enhanced voice activity detection
  const detectVoiceActivity = useCallback(() => {
    if (!analyserRef.current || !isCallActiveRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
    const isVoiceActive = average > 15; // Adjusted threshold for better detection
    
    if (isVoiceActive && isCallActiveRef.current) {
      // Clear silence timeout
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
        silenceTimeoutRef.current = null;
      }
      
      // Update energy level
      setState(prev => ({ 
        ...prev, 
        energy: Math.min(100, prev.energy + 2),
        connectionQuality: Math.min(100, prev.connectionQuality + 1)
      }));
    } else {
      // Set silence timeout
      if (!silenceTimeoutRef.current && isCallActiveRef.current) {
        silenceTimeoutRef.current = setTimeout(() => {
          if (isCallActiveRef.current) {
            setState(prev => ({ 
              ...prev, 
              energy: Math.max(0, prev.energy - 1),
              connectionQuality: Math.max(50, prev.connectionQuality - 0.5)
            }));
          }
        }, 2000);
      }
    }

    if (isCallActiveRef.current) {
      animationRef.current = requestAnimationFrame(detectVoiceActivity);
    }
  }, []);

  // Enhanced user message handling
  const handleUserMessage = useCallback(async (message: string) => {
    if (!message.trim() || !isCallActiveRef.current) return;

    console.log('💬 Processing user message:', message);
    
    setState(prev => ({ 
      ...prev, 
      isProcessing: true,
      isListening: false,
      currentMessage: ''
    }));

    // Add user message to history
    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      content: message,
      sender: 'user',
      timestamp: new Date(),
      emotion: detectEmotion(message),
      topics: extractTopics(message)
    };

    setState(prev => ({
      ...prev,
      conversationHistory: [...prev.conversationHistory, userMessage]
    }));

    try {
      // Create enhanced chat context with proper user name
      const chatContext: ChatContext = {
        character,
        userPreferences: {
          ...userPreferences,
          preferredName: userPreferences.petName || userPreferences.preferredName || 'friend'
        },
        conversationHistory: [...state.conversationHistory, userMessage],
        relationshipLevel: state.relationshipLevel,
        timeOfDay: getTimeOfDay(),
        sessionMemory: state.sessionMemory
      };

      // Generate AI response with enhanced personality
      const aiResponse = await personalityAI.generateResponse(message, chatContext);
      
      console.log('🤖 AI Response:', aiResponse);

      // Check if call is still active before processing response
      if (!isCallActiveRef.current) {
        console.log('🛑 Call ended, stopping AI response processing');
        return;
      }

      // Add AI response to history
      const aiMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        content: aiResponse,
        sender: 'ai',
        timestamp: new Date(),
        emotion: detectEmotion(aiResponse),
        topics: extractTopics(aiResponse)
      };

      setState(prev => ({
        ...prev,
        conversationHistory: [...prev.conversationHistory, aiMessage],
        relationshipLevel: Math.min(100, prev.relationshipLevel + 1),
        mood: detectMood(aiResponse)
      }));

      // Speak the response with enhanced voice settings
      await speakAIResponse(aiResponse, message);

    } catch (error) {
      console.error('❌ Error processing message:', error);
      if (isCallActiveRef.current) {
        toast({
          title: "Processing Error",
          description: "Sorry, I had trouble understanding that. Could you try again?",
          variant: "destructive"
        });
      }
    } finally {
      if (isCallActiveRef.current) {
        setState(prev => ({ 
          ...prev, 
          isProcessing: false,
          isListening: true 
        }));
      }
    }
  }, [character, userPreferences, state.conversationHistory, state.relationshipLevel, state.sessionMemory]);

  // Enhanced AI response speaking with natural voice settings
  const speakAIResponse = useCallback(async (response: string, userMessage: string) => {
    if (!isCallActiveRef.current) {
      console.log('🛑 Call ended, stopping AI speech');
      return;
    }

    try {
      setState(prev => ({ ...prev, isSpeaking: true }));
      
      // Determine voice context based on conversation
      const voiceContext = determineVoiceContext(response, userMessage);
      const voiceSettings = getNaturalVoiceSettings(voiceContext);
      
      console.log('🎤 Speaking with context:', voiceContext, 'Settings:', voiceSettings);
      
      // Stop any current TTS
      stopAllTTS();
      
      // Speak with enhanced settings
      await speakText(response, character.voice?.voice_id, {
        modelId: 'eleven_multilingual_v2',
        voiceSettings
      });
      
      console.log('✅ Speech completed');
      
    } catch (error) {
      console.error('❌ Speech error:', error);
      if (isCallActiveRef.current) {
        toast({
          title: "Speech Error",
          description: "Sorry, I had trouble speaking. Let me try again.",
          variant: "destructive"
        });
      }
    } finally {
      if (isCallActiveRef.current) {
        setState(prev => ({ ...prev, isSpeaking: false }));
        
        // Resume listening after a brief pause
        setTimeout(() => {
          if (!state.isListening && isCallActiveRef.current) {
            startListening();
          }
        }, 1000);
      }
    }
  }, [character.voice?.voice_id]);

  // Determine voice context for natural speech
  const determineVoiceContext = (response: string, userMessage: string): 'excited' | 'calm' | 'intimate' | 'playful' | 'professional' => {
    const lowerResponse = response.toLowerCase();
    const lowerUserMessage = userMessage.toLowerCase();
    
    if (lowerResponse.includes('love') || lowerResponse.includes('adore') || lowerResponse.includes('heart')) {
      return 'intimate';
    }
    if (lowerResponse.includes('!') || lowerResponse.includes('amazing') || lowerResponse.includes('wonderful')) {
      return 'excited';
    }
    if (lowerResponse.includes('haha') || lowerResponse.includes('funny') || lowerResponse.includes('laugh')) {
      return 'playful';
    }
    if (lowerResponse.includes('work') || lowerResponse.includes('business') || lowerResponse.includes('professional')) {
      return 'professional';
    }
    return 'calm';
  };

  // Enhanced emotion detection
  const detectEmotion = (text: string): string => {
    const lowerText = text.toLowerCase();
    
    if (lowerText.includes('love') || lowerText.includes('adore') || lowerText.includes('heart')) return 'romantic';
    if (lowerText.includes('happy') || lowerText.includes('excited') || lowerText.includes('amazing')) return 'happy';
    if (lowerText.includes('sad') || lowerText.includes('upset') || lowerText.includes('worried')) return 'sad';
    if (lowerText.includes('angry') || lowerText.includes('frustrated') || lowerText.includes('mad')) return 'angry';
    if (lowerText.includes('funny') || lowerText.includes('laugh') || lowerText.includes('haha')) return 'playful';
    
    return 'neutral';
  };

  // Enhanced mood detection
  const detectMood = (text: string): string => {
    const lowerText = text.toLowerCase();
    
    if (lowerText.includes('love') || lowerText.includes('adore')) return 'romantic';
    if (lowerText.includes('happy') || lowerText.includes('excited')) return 'happy';
    if (lowerText.includes('calm') || lowerText.includes('peaceful')) return 'calm';
    if (lowerText.includes('playful') || lowerText.includes('fun')) return 'playful';
    
    return 'neutral';
  };

  // Extract topics from text
  const extractTopics = (text: string): string[] => {
    const lowerText = text.toLowerCase();
    const topics = [];
    
    if (lowerText.includes('work') || lowerText.includes('job')) topics.push('work');
    if (lowerText.includes('family') || lowerText.includes('parents')) topics.push('family');
    if (lowerText.includes('friend') || lowerText.includes('friends')) topics.push('friends');
    if (lowerText.includes('love') || lowerText.includes('relationship')) topics.push('love');
    if (lowerText.includes('dream') || lowerText.includes('future')) topics.push('dreams');
    if (lowerText.includes('hobby') || lowerText.includes('fun')) topics.push('hobbies');
    
    return topics;
  };

  // Get time of day
  const getTimeOfDay = (): 'morning' | 'afternoon' | 'evening' => {
    const hour = new Date().getHours();
    if (hour < 12) return 'morning';
    if (hour < 18) return 'afternoon';
    return 'evening';
  };

  // Enhanced listening control
  const startListening = useCallback(() => {
    if (state.isSpeaking || state.isProcessing || !isCallActiveRef.current) return;
    
    if (recognitionRef.current) {
      try {
        recognitionRef.current.start();
        console.log('🎤 Started listening');
      } catch (error) {
        console.error('❌ Failed to start listening:', error);
      }
    }
  }, [state.isSpeaking, state.isProcessing]);

  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      console.log('🛑 Stopped listening');
    }
  }, []);

  // Enhanced call control with proper cleanup
  const startCall = useCallback(async () => {
    console.log('📞 Starting voice call...');
    isCallActiveRef.current = true;
    
    // Setup audio context
    const audioSetup = await setupAudioContext();
    if (!audioSetup) {
      toast({
        title: "Audio Setup Failed",
        description: "Could not access microphone. Please check permissions.",
        variant: "destructive"
      });
      return;
    }

    // Setup speech recognition
    const recognitionSetup = setupSpeechRecognition();
    if (!recognitionSetup) return;

    // Start listening
    startListening();
    
    // Start voice activity detection
    detectVoiceActivity();
    
    toast({
      title: "Call Started",
      description: `You're now talking with ${character.name}!`,
    });
  }, [setupAudioContext, setupSpeechRecognition, startListening, detectVoiceActivity, character.name]);

  const endCall = useCallback(() => {
    console.log('📞 Ending voice call...');
    isCallActiveRef.current = false;
    
    // Stop all audio immediately
    stopAllTTS();
    stopListening();
    
    // Cleanup all audio resources
    if (audioContextRef.current) {
      try {
        audioContextRef.current.close();
        audioContextRef.current = null;
      } catch (error) {
        console.error('❌ Error closing audio context:', error);
      }
    }
    
    if (analyserRef.current) {
      analyserRef.current = null;
    }
    
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }
    
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }
    
    if (conversationTimeoutRef.current) {
      clearTimeout(conversationTimeoutRef.current);
      conversationTimeoutRef.current = null;
    }
    
    // Stop speech recognition
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (error) {
        console.error('❌ Error stopping speech recognition:', error);
      }
    }
    
    // Reset state
    setState({
      isListening: false,
      isProcessing: false,
      isSpeaking: false,
      currentMessage: '',
      conversationHistory: [],
      relationshipLevel: 50,
      sessionMemory: {},
      mood: 'neutral',
      energy: 70,
      connectionQuality: 95
    });
    
    console.log('✅ Call cleanup completed');
    onEndCall();
  }, [stopListening, onEndCall]);

  // Initialize call on mount
  useEffect(() => {
    startCall();
    
    return () => {
      endCall();
    };
  }, [startCall, endCall]);

  // Enhanced UI rendering
  return (
    <div className={`flex flex-col h-full bg-gradient-to-br from-purple-900 via-blue-900 to-indigo-900 text-white ${className}`}>
      {/* Header */}
      <div className="flex items-center justify-between p-4 border-b border-white/20">
        <div className="flex items-center space-x-3">
          <div className="w-12 h-12 rounded-full bg-gradient-to-r from-pink-500 to-purple-500 flex items-center justify-center">
            <span className="text-xl font-bold">{character.name[0]}</span>
          </div>
          <div>
            <h2 className="text-xl font-semibold">{character.name}</h2>
            <p className="text-sm text-white/70">Voice Call Active</p>
          </div>
        </div>
        
        <Button
          onClick={endCall}
          variant="destructive"
          size="sm"
          className="bg-red-600 hover:bg-red-700"
        >
          <PhoneOff className="w-4 h-4 mr-2" />
          End Call
        </Button>
      </div>

      {/* Main Content */}
      <div className="flex-1 flex flex-col p-4 space-y-4">
        {/* Status Indicators */}
        <div className="grid grid-cols-2 md:grid-cols-4 gap-4">
          <Card className="p-3 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2">
              <Mic className={`w-4 h-4 ${state.isListening ? 'text-green-400' : 'text-gray-400'}`} />
              <span className="text-sm">Listening</span>
            </div>
            <div className="text-xs text-white/70 mt-1">
              {state.isListening ? 'Active' : 'Inactive'}
            </div>
          </Card>
          
          <Card className="p-3 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2">
              <Volume2 className={`w-4 h-4 ${state.isSpeaking ? 'text-blue-400' : 'text-gray-400'}`} />
              <span className="text-sm">Speaking</span>
            </div>
            <div className="text-xs text-white/70 mt-1">
              {state.isSpeaking ? 'Active' : 'Inactive'}
            </div>
          </Card>
          
          <Card className="p-3 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2">
              <Heart className="w-4 h-4 text-pink-400" />
              <span className="text-sm">Relationship</span>
            </div>
            <div className="text-xs text-white/70 mt-1">
              {state.relationshipLevel}%
            </div>
          </Card>
          
          <Card className="p-3 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2">
              <Activity className="w-4 h-4 text-green-400" />
              <span className="text-sm">Connection</span>
            </div>
            <div className="text-xs text-white/70 mt-1">
              {state.connectionQuality}%
            </div>
          </Card>
        </div>

        {/* Current Message Display */}
        {state.currentMessage && (
          <Card className="p-4 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2 mb-2">
              <MessageCircle className="w-4 h-4 text-blue-400" />
              <span className="text-sm font-medium">You said:</span>
            </div>
            <p className="text-white/90">{state.currentMessage}</p>
          </Card>
        )}

        {/* Processing Indicator */}
        {state.isProcessing && (
          <Card className="p-4 bg-white/10 backdrop-blur-sm">
            <div className="flex items-center space-x-2">
              <Loader2 className="w-4 h-4 animate-spin text-blue-400" />
              <span className="text-sm">Processing your message...</span>
            </div>
          </Card>
        )}

        {/* Conversation History */}
        <div className="flex-1 overflow-y-auto space-y-2">
          {state.conversationHistory.map((message) => (
            <Card
              key={message.id}
              className={`p-3 ${
                message.sender === 'user' 
                  ? 'bg-blue-500/20 ml-8' 
                  : 'bg-purple-500/20 mr-8'
              } backdrop-blur-sm`}
            >
              <div className="flex items-center space-x-2 mb-1">
                {message.sender === 'user' ? (
                  <Mic className="w-3 h-3 text-blue-400" />
                ) : (
                  <Volume2 className="w-3 h-3 text-purple-400" />
                )}
                <span className="text-xs text-white/70">
                  {message.sender === 'user' ? 'You' : character.name}
                </span>
                {message.emotion && (
                  <Badge variant="secondary" className="text-xs">
                    {message.emotion}
                  </Badge>
                )}
              </div>
              <p className="text-sm text-white/90">{message.content}</p>
            </Card>
          ))}
        </div>

        {/* Controls */}
        <div className="flex items-center justify-center space-x-4">
          <Button
            onClick={state.isListening ? stopListening : startListening}
            disabled={state.isSpeaking || state.isProcessing}
            className={`${
              state.isListening 
                ? 'bg-red-600 hover:bg-red-700' 
                : 'bg-green-600 hover:bg-green-700'
            }`}
          >
            {state.isListening ? (
              <MicOff className="w-4 h-4 mr-2" />
            ) : (
              <Mic className="w-4 h-4 mr-2" />
            )}
            {state.isListening ? 'Stop Listening' : 'Start Listening'}
          </Button>
          
          <Button
            onClick={() => stopAllTTS()}
            disabled={!state.isSpeaking}
            variant="outline"
            className="border-white/30 text-white hover:bg-white/10"
          >
            <VolumeX className="w-4 h-4 mr-2" />
            Stop Speaking
          </Button>
        </div>
      </div>
    </div>
  );
};
